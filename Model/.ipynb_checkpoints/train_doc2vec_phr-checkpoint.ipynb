{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import gensim\n",
    "import gensim \n",
    "import logging\n",
    "import io\n",
    "from pathlib import Path\n",
    "# read in some helpful libraries\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from gensim.test.utils import get_tmpfile\n",
    "import re\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram is the model to detect and group words into phrases\n",
    "def prepare_bigram(corpus_file_name):\n",
    "    # Read file and return sentences as a list of lists of string\n",
    "    # where each list of string represents a sentence\n",
    "    sentences = Text8Corpus(('../corpus/merged.txt'))\n",
    "    \n",
    "    # Detect and group phrases\n",
    "    phrases = Phrases(sentences, min_count=1, threshold=5,common_terms=['in','of'])\n",
    "    bigram = Phraser(phrases)\n",
    "    return bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrinsic_evaluate(data,model):\n",
    "    '''\n",
    "    data contains 3 columns: Term1, Term2, score which is the relatedness/similarity score\n",
    "    between two terms judged by clinical experts\n",
    "    the output is the correlation coefficient between the predicting score by the model and the expert judgement score\n",
    "    '''\n",
    "    has_vocab_condition = (umn_sim['Term1'].isin(model.wv.vocab)) & (umn_sim['Term2'].isin(model.wv.vocab))\n",
    "    data = data[has_vocab_condition]\n",
    "    data['pred_score']=data[['Term1','Term2']].apply(lambda row: cosine_sim(row['Term1'],row['Term2'],model),axis=1)\n",
    "    return data['pred_score'].corr(data['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(w1,w2, model):\n",
    "    vectA = np.array(model[w1])\n",
    "    vectB = np.array(model[w2])\n",
    "    return (vectA.dot(vectB))/(np.linalg.norm(vectA) * np.linalg.norm(vectB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname, bigram, tokens_only=False):\n",
    "    with open(fname,encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = re.sub(\"-\",\"_\",str(line))\n",
    "            if tokens_only:\n",
    "                yield bigram[gensim.utils.simple_preprocess(line)]\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(bigram[gensim.utils.simple_preprocess(line)], [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-25 01:29:40,869 : INFO : collecting all words and their counts\n",
      "2018-11-25 01:29:40,871 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2018-11-25 01:29:47,507 : INFO : collected 1615674 word types from a corpus of 4752594 words (unigram + bigrams) and 476 sentences\n",
      "2018-11-25 01:29:47,508 : INFO : using 1615674 counts as vocab in Phrases<0 vocab, min_count=1, threshold=5, max_vocab_size=40000000>\n",
      "2018-11-25 01:29:47,508 : INFO : source_vocab length 1615674\n",
      "2018-11-25 01:29:49,476 : INFO : Phraser added 50000 phrasegrams\n",
      "2018-11-25 01:29:52,432 : INFO : Phraser added 100000 phrasegrams\n",
      "2018-11-25 01:29:58,715 : INFO : Phraser added 150000 phrasegrams\n",
      "2018-11-25 01:29:59,973 : INFO : Phraser built with 155098 155098 phrasegrams\n",
      "2018-11-25 01:30:14,479 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2018-11-25 01:30:14,479 : INFO : collecting all words and their counts\n",
      "2018-11-25 01:30:14,480 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2018-11-25 01:30:14,507 : INFO : PROGRESS: at example #10000, processed 103470 words (3884108/s), 11716 word types, 10000 tags\n",
      "2018-11-25 01:30:14,533 : INFO : PROGRESS: at example #20000, processed 200644 words (3837640/s), 17368 word types, 20000 tags\n",
      "2018-11-25 01:30:14,561 : INFO : PROGRESS: at example #30000, processed 310612 words (3804669/s), 23928 word types, 30000 tags\n",
      "2018-11-25 01:30:14,588 : INFO : PROGRESS: at example #40000, processed 409379 words (3711959/s), 28012 word types, 40000 tags\n",
      "2018-11-25 01:30:14,615 : INFO : PROGRESS: at example #50000, processed 512651 words (3920839/s), 31390 word types, 50000 tags\n",
      "2018-11-25 01:30:14,642 : INFO : PROGRESS: at example #60000, processed 610904 words (3824978/s), 34597 word types, 60000 tags\n",
      "2018-11-25 01:30:14,667 : INFO : PROGRESS: at example #70000, processed 703860 words (3778038/s), 37133 word types, 70000 tags\n",
      "2018-11-25 01:30:14,694 : INFO : PROGRESS: at example #80000, processed 807342 words (3889298/s), 40747 word types, 80000 tags\n",
      "2018-11-25 01:30:14,736 : INFO : PROGRESS: at example #90000, processed 994518 words (4569103/s), 52478 word types, 90000 tags\n",
      "2018-11-25 01:30:14,787 : INFO : PROGRESS: at example #100000, processed 1257365 words (5238255/s), 62955 word types, 100000 tags\n",
      "2018-11-25 01:30:14,835 : INFO : PROGRESS: at example #110000, processed 1513037 words (5310693/s), 69855 word types, 110000 tags\n",
      "2018-11-25 01:30:14,882 : INFO : PROGRESS: at example #120000, processed 1750960 words (5146836/s), 75521 word types, 120000 tags\n",
      "2018-11-25 01:30:14,923 : INFO : PROGRESS: at example #130000, processed 1945574 words (4828233/s), 80457 word types, 130000 tags\n",
      "2018-11-25 01:30:14,972 : INFO : PROGRESS: at example #140000, processed 2156152 words (4271804/s), 85828 word types, 140000 tags\n",
      "2018-11-25 01:30:15,020 : INFO : PROGRESS: at example #150000, processed 2369134 words (4572804/s), 90430 word types, 150000 tags\n",
      "2018-11-25 01:30:15,063 : INFO : PROGRESS: at example #160000, processed 2572401 words (4783879/s), 94439 word types, 160000 tags\n",
      "2018-11-25 01:30:15,112 : INFO : PROGRESS: at example #170000, processed 2812343 words (4950054/s), 98700 word types, 170000 tags\n",
      "2018-11-25 01:30:15,210 : INFO : PROGRESS: at example #180000, processed 3365529 words (5675788/s), 109919 word types, 180000 tags\n",
      "2018-11-25 01:30:15,259 : INFO : collected 114691 word types and 181150 unique tags from a corpus of 181150 examples and 3673661 words\n",
      "2018-11-25 01:30:15,260 : INFO : Loading a fresh vocabulary\n",
      "2018-11-25 01:30:15,473 : INFO : min_count=2 retains 76553 unique words (66% of original 114691, drops 38138)\n",
      "2018-11-25 01:30:15,474 : INFO : min_count=2 leaves 3635523 word corpus (98% of original 3673661, drops 38138)\n",
      "2018-11-25 01:30:15,637 : INFO : deleting the raw counts dictionary of 114691 items\n",
      "2018-11-25 01:30:15,640 : INFO : sample=0.001 downsamples 35 most-common words\n",
      "2018-11-25 01:30:15,640 : INFO : downsampling leaves estimated 2914771 word corpus (80.2% of prior 3635523)\n",
      "2018-11-25 01:30:15,810 : INFO : estimated required memory for 76553 words and 250 dimensions: 372532500 bytes\n",
      "2018-11-25 01:30:15,811 : INFO : resetting layer weights\n",
      "2018-11-25 01:30:18,641 : INFO : training model with 3 workers on 76553 vocabulary and 250 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2018-11-25 01:30:19,689 : INFO : EPOCH 1 - PROGRESS: at 17.14% examples, 273223 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:20,709 : INFO : EPOCH 1 - PROGRESS: at 35.25% examples, 280186 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:21,725 : INFO : EPOCH 1 - PROGRESS: at 52.22% examples, 318383 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:22,752 : INFO : EPOCH 1 - PROGRESS: at 67.09% examples, 373893 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:23,762 : INFO : EPOCH 1 - PROGRESS: at 82.80% examples, 395945 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:24,768 : INFO : EPOCH 1 - PROGRESS: at 96.38% examples, 421005 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:25,230 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:25,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:25,236 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:25,236 : INFO : EPOCH - 1 : training on 3673661 raw words (2949535 effective words) took 6.6s, 447652 effective words/s\n",
      "2018-11-25 01:30:26,393 : INFO : EPOCH 2 - PROGRESS: at 17.14% examples, 269109 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:27,424 : INFO : EPOCH 2 - PROGRESS: at 34.61% examples, 272065 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:28,432 : INFO : EPOCH 2 - PROGRESS: at 51.63% examples, 308281 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:29,440 : INFO : EPOCH 2 - PROGRESS: at 66.03% examples, 365674 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:30,462 : INFO : EPOCH 2 - PROGRESS: at 81.31% examples, 385476 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:31,474 : INFO : EPOCH 2 - PROGRESS: at 96.20% examples, 404524 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:32,048 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:32,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:32,054 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:32,055 : INFO : EPOCH - 2 : training on 3673661 raw words (2950072 effective words) took 6.7s, 439020 effective words/s\n",
      "2018-11-25 01:30:33,096 : INFO : EPOCH 3 - PROGRESS: at 17.14% examples, 275371 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:34,131 : INFO : EPOCH 3 - PROGRESS: at 35.39% examples, 279228 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:35,142 : INFO : EPOCH 3 - PROGRESS: at 52.43% examples, 320749 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:36,156 : INFO : EPOCH 3 - PROGRESS: at 67.09% examples, 374800 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:37,188 : INFO : EPOCH 3 - PROGRESS: at 82.75% examples, 395189 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:38,198 : INFO : EPOCH 3 - PROGRESS: at 96.38% examples, 419983 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:38,667 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:38,667 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:38,672 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:38,673 : INFO : EPOCH - 3 : training on 3673661 raw words (2950719 effective words) took 6.6s, 446392 effective words/s\n",
      "2018-11-25 01:30:39,724 : INFO : EPOCH 4 - PROGRESS: at 17.14% examples, 272445 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:40,744 : INFO : EPOCH 4 - PROGRESS: at 34.61% examples, 275160 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:41,759 : INFO : EPOCH 4 - PROGRESS: at 51.47% examples, 307353 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:42,783 : INFO : EPOCH 4 - PROGRESS: at 65.45% examples, 361476 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:43,784 : INFO : EPOCH 4 - PROGRESS: at 81.00% examples, 385316 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:44,792 : INFO : EPOCH 4 - PROGRESS: at 96.00% examples, 404841 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:45,361 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:45,366 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:45,367 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:45,367 : INFO : EPOCH - 4 : training on 3673661 raw words (2950013 effective words) took 6.7s, 440977 effective words/s\n",
      "2018-11-25 01:30:46,440 : INFO : EPOCH 5 - PROGRESS: at 17.14% examples, 267715 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:47,463 : INFO : EPOCH 5 - PROGRESS: at 34.61% examples, 272392 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:48,488 : INFO : EPOCH 5 - PROGRESS: at 51.63% examples, 306893 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:49,498 : INFO : EPOCH 5 - PROGRESS: at 66.03% examples, 364005 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:50,529 : INFO : EPOCH 5 - PROGRESS: at 81.31% examples, 383436 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:51,542 : INFO : EPOCH 5 - PROGRESS: at 96.20% examples, 402676 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:52,100 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:52,103 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:52,104 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:52,105 : INFO : EPOCH - 5 : training on 3673661 raw words (2949566 effective words) took 6.7s, 438327 effective words/s\n",
      "2018-11-25 01:30:53,168 : INFO : EPOCH 6 - PROGRESS: at 17.12% examples, 268991 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:54,204 : INFO : EPOCH 6 - PROGRESS: at 34.61% examples, 271392 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:55,207 : INFO : EPOCH 6 - PROGRESS: at 51.47% examples, 305696 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:56,222 : INFO : EPOCH 6 - PROGRESS: at 65.45% examples, 360923 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:30:57,232 : INFO : EPOCH 6 - PROGRESS: at 80.42% examples, 380821 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:58,240 : INFO : EPOCH 6 - PROGRESS: at 93.64% examples, 390129 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:30:59,020 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:30:59,024 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:30:59,025 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:30:59,025 : INFO : EPOCH - 6 : training on 3673661 raw words (2949337 effective words) took 6.9s, 426572 effective words/s\n",
      "2018-11-25 01:31:00,068 : INFO : EPOCH 7 - PROGRESS: at 17.14% examples, 274541 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:01,103 : INFO : EPOCH 7 - PROGRESS: at 35.39% examples, 278968 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:02,115 : INFO : EPOCH 7 - PROGRESS: at 52.22% examples, 317845 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:03,121 : INFO : EPOCH 7 - PROGRESS: at 66.83% examples, 373094 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:04,122 : INFO : EPOCH 7 - PROGRESS: at 81.97% examples, 392842 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:05,123 : INFO : EPOCH 7 - PROGRESS: at 96.34% examples, 418511 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:05,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:05,616 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:05,617 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:05,617 : INFO : EPOCH - 7 : training on 3673661 raw words (2949186 effective words) took 6.6s, 447809 effective words/s\n",
      "2018-11-25 01:31:06,662 : INFO : EPOCH 8 - PROGRESS: at 17.14% examples, 274384 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:07,692 : INFO : EPOCH 8 - PROGRESS: at 34.61% examples, 274951 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:08,722 : INFO : EPOCH 8 - PROGRESS: at 51.63% examples, 308459 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:09,734 : INFO : EPOCH 8 - PROGRESS: at 66.03% examples, 365184 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:10,759 : INFO : EPOCH 8 - PROGRESS: at 81.31% examples, 384812 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:11,772 : INFO : EPOCH 8 - PROGRESS: at 96.20% examples, 403897 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:12,335 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:12,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:12,340 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:12,341 : INFO : EPOCH - 8 : training on 3673661 raw words (2949707 effective words) took 6.7s, 439199 effective words/s\n",
      "2018-11-25 01:31:13,402 : INFO : EPOCH 9 - PROGRESS: at 17.14% examples, 270948 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:14,421 : INFO : EPOCH 9 - PROGRESS: at 34.61% examples, 274486 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:15,432 : INFO : EPOCH 9 - PROGRESS: at 51.82% examples, 312513 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:16,445 : INFO : EPOCH 9 - PROGRESS: at 66.31% examples, 368509 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:17,450 : INFO : EPOCH 9 - PROGRESS: at 81.97% examples, 392258 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:18,452 : INFO : EPOCH 9 - PROGRESS: at 96.33% examples, 415621 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:18,950 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:18,954 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:18,955 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:18,956 : INFO : EPOCH - 9 : training on 3673661 raw words (2949634 effective words) took 6.6s, 446550 effective words/s\n",
      "2018-11-25 01:31:20,022 : INFO : EPOCH 10 - PROGRESS: at 17.04% examples, 268178 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:21,039 : INFO : EPOCH 10 - PROGRESS: at 35.25% examples, 277996 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:22,041 : INFO : EPOCH 10 - PROGRESS: at 52.43% examples, 320804 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:23,050 : INFO : EPOCH 10 - PROGRESS: at 67.09% examples, 375238 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:24,076 : INFO : EPOCH 10 - PROGRESS: at 82.80% examples, 395885 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:25,086 : INFO : EPOCH 10 - PROGRESS: at 96.57% examples, 426617 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:25,508 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:25,512 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:25,513 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:25,513 : INFO : EPOCH - 10 : training on 3673661 raw words (2949147 effective words) took 6.6s, 450205 effective words/s\n",
      "2018-11-25 01:31:26,598 : INFO : EPOCH 11 - PROGRESS: at 17.12% examples, 264561 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:27,615 : INFO : EPOCH 11 - PROGRESS: at 34.61% examples, 271497 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:28,650 : INFO : EPOCH 11 - PROGRESS: at 51.63% examples, 305278 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:29,661 : INFO : EPOCH 11 - PROGRESS: at 65.40% examples, 358467 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:30,675 : INFO : EPOCH 11 - PROGRESS: at 80.69% examples, 380066 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:31,705 : INFO : EPOCH 11 - PROGRESS: at 95.70% examples, 398851 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:32,279 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:32,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:32,284 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:32,285 : INFO : EPOCH - 11 : training on 3673661 raw words (2949999 effective words) took 6.8s, 436154 effective words/s\n",
      "2018-11-25 01:31:33,330 : INFO : EPOCH 12 - PROGRESS: at 17.14% examples, 274856 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:34,352 : INFO : EPOCH 12 - PROGRESS: at 34.61% examples, 276229 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:35,353 : INFO : EPOCH 12 - PROGRESS: at 51.63% examples, 312275 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:36,368 : INFO : EPOCH 12 - PROGRESS: at 65.13% examples, 362106 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:37,370 : INFO : EPOCH 12 - PROGRESS: at 79.63% examples, 379380 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:38,381 : INFO : EPOCH 12 - PROGRESS: at 94.37% examples, 396878 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:39,051 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:39,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:39,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:39,056 : INFO : EPOCH - 12 : training on 3673661 raw words (2949334 effective words) took 6.8s, 436102 effective words/s\n",
      "2018-11-25 01:31:40,107 : INFO : EPOCH 13 - PROGRESS: at 17.14% examples, 272759 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:41,131 : INFO : EPOCH 13 - PROGRESS: at 35.25% examples, 279396 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:42,146 : INFO : EPOCH 13 - PROGRESS: at 52.22% examples, 317780 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:43,180 : INFO : EPOCH 13 - PROGRESS: at 67.08% examples, 372703 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:44,197 : INFO : EPOCH 13 - PROGRESS: at 82.51% examples, 392862 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:45,202 : INFO : EPOCH 13 - PROGRESS: at 96.33% examples, 414281 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:45,721 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:45,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:45,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:45,726 : INFO : EPOCH - 13 : training on 3673661 raw words (2949709 effective words) took 6.7s, 442744 effective words/s\n",
      "2018-11-25 01:31:46,820 : INFO : EPOCH 14 - PROGRESS: at 17.14% examples, 262059 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:47,849 : INFO : EPOCH 14 - PROGRESS: at 35.25% examples, 273034 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:48,855 : INFO : EPOCH 14 - PROGRESS: at 52.22% examples, 313784 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:49,881 : INFO : EPOCH 14 - PROGRESS: at 67.09% examples, 369634 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:50,907 : INFO : EPOCH 14 - PROGRESS: at 82.80% examples, 391242 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:51,913 : INFO : EPOCH 14 - PROGRESS: at 96.46% examples, 420142 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:52,341 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:52,344 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:52,345 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:52,346 : INFO : EPOCH - 14 : training on 3673661 raw words (2948412 effective words) took 6.6s, 445945 effective words/s\n",
      "2018-11-25 01:31:53,390 : INFO : EPOCH 15 - PROGRESS: at 17.14% examples, 274028 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:54,391 : INFO : EPOCH 15 - PROGRESS: at 34.61% examples, 278685 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:31:55,393 : INFO : EPOCH 15 - PROGRESS: at 52.02% examples, 319251 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:56,403 : INFO : EPOCH 15 - PROGRESS: at 66.03% examples, 370411 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:57,411 : INFO : EPOCH 15 - PROGRESS: at 81.00% examples, 388728 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:58,434 : INFO : EPOCH 15 - PROGRESS: at 95.75% examples, 405366 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:31:59,024 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:31:59,029 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:31:59,030 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:31:59,031 : INFO : EPOCH - 15 : training on 3673661 raw words (2949093 effective words) took 6.7s, 441543 effective words/s\n",
      "2018-11-25 01:32:00,042 : INFO : EPOCH 16 - PROGRESS: at 16.11% examples, 265740 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:01,045 : INFO : EPOCH 16 - PROGRESS: at 33.47% examples, 274180 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:02,053 : INFO : EPOCH 16 - PROGRESS: at 49.92% examples, 294835 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:03,082 : INFO : EPOCH 16 - PROGRESS: at 64.61% examples, 360628 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:04,097 : INFO : EPOCH 16 - PROGRESS: at 80.42% examples, 385533 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:05,109 : INFO : EPOCH 16 - PROGRESS: at 95.70% examples, 406152 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:05,668 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:05,671 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:05,673 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:05,673 : INFO : EPOCH - 16 : training on 3673661 raw words (2949949 effective words) took 6.6s, 444598 effective words/s\n",
      "2018-11-25 01:32:06,741 : INFO : EPOCH 17 - PROGRESS: at 17.14% examples, 268094 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:07,759 : INFO : EPOCH 17 - PROGRESS: at 35.25% examples, 277744 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:08,765 : INFO : EPOCH 17 - PROGRESS: at 52.22% examples, 317547 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:09,798 : INFO : EPOCH 17 - PROGRESS: at 67.09% examples, 372533 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:10,808 : INFO : EPOCH 17 - PROGRESS: at 82.80% examples, 394819 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:11,810 : INFO : EPOCH 17 - PROGRESS: at 96.37% examples, 418982 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:12,268 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:12,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:12,272 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:12,273 : INFO : EPOCH - 17 : training on 3673661 raw words (2949513 effective words) took 6.6s, 447331 effective words/s\n",
      "2018-11-25 01:32:13,309 : INFO : EPOCH 18 - PROGRESS: at 17.14% examples, 276489 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:14,329 : INFO : EPOCH 18 - PROGRESS: at 34.61% examples, 277352 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:15,334 : INFO : EPOCH 18 - PROGRESS: at 51.82% examples, 315226 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:16,363 : INFO : EPOCH 18 - PROGRESS: at 66.31% examples, 369474 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:17,368 : INFO : EPOCH 18 - PROGRESS: at 81.73% examples, 391387 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:18,372 : INFO : EPOCH 18 - PROGRESS: at 96.31% examples, 410084 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:18,929 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:18,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:18,934 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:18,935 : INFO : EPOCH - 18 : training on 3673661 raw words (2949191 effective words) took 6.7s, 443106 effective words/s\n",
      "2018-11-25 01:32:19,983 : INFO : EPOCH 19 - PROGRESS: at 17.14% examples, 273528 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:21,000 : INFO : EPOCH 19 - PROGRESS: at 35.25% examples, 280604 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:22,013 : INFO : EPOCH 19 - PROGRESS: at 52.72% examples, 324462 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:23,014 : INFO : EPOCH 19 - PROGRESS: at 67.09% examples, 376752 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:24,025 : INFO : EPOCH 19 - PROGRESS: at 82.80% examples, 398354 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:25,026 : INFO : EPOCH 19 - PROGRESS: at 96.52% examples, 428111 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:25,450 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:25,454 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:25,455 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:25,455 : INFO : EPOCH - 19 : training on 3673661 raw words (2949778 effective words) took 6.5s, 452918 effective words/s\n",
      "2018-11-25 01:32:26,509 : INFO : EPOCH 20 - PROGRESS: at 17.14% examples, 271763 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:27,517 : INFO : EPOCH 20 - PROGRESS: at 34.61% examples, 276557 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:28,543 : INFO : EPOCH 20 - PROGRESS: at 51.63% examples, 310037 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:29,546 : INFO : EPOCH 20 - PROGRESS: at 66.03% examples, 367523 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:30,562 : INFO : EPOCH 20 - PROGRESS: at 81.31% examples, 387450 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:31,563 : INFO : EPOCH 20 - PROGRESS: at 96.00% examples, 405688 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:32,139 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:32,144 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:32,145 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:32,145 : INFO : EPOCH - 20 : training on 3673661 raw words (2950395 effective words) took 6.7s, 441397 effective words/s\n",
      "2018-11-25 01:32:33,218 : INFO : EPOCH 21 - PROGRESS: at 17.04% examples, 267018 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:34,261 : INFO : EPOCH 21 - PROGRESS: at 34.61% examples, 269803 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:35,262 : INFO : EPOCH 21 - PROGRESS: at 50.85% examples, 296698 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:36,270 : INFO : EPOCH 21 - PROGRESS: at 65.13% examples, 358503 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:37,301 : INFO : EPOCH 21 - PROGRESS: at 80.42% examples, 379000 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:38,320 : INFO : EPOCH 21 - PROGRESS: at 95.75% examples, 400000 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:38,892 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:38,895 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:38,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:38,897 : INFO : EPOCH - 21 : training on 3673661 raw words (2950306 effective words) took 6.7s, 437422 effective words/s\n",
      "2018-11-25 01:32:39,936 : INFO : EPOCH 22 - PROGRESS: at 17.14% examples, 276269 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:40,961 : INFO : EPOCH 22 - PROGRESS: at 35.25% examples, 281000 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:41,965 : INFO : EPOCH 22 - PROGRESS: at 52.43% examples, 322980 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:42,973 : INFO : EPOCH 22 - PROGRESS: at 67.09% examples, 377230 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:43,983 : INFO : EPOCH 22 - PROGRESS: at 82.75% examples, 398812 words/s, in_qsize 5, out_qsize 1\n",
      "2018-11-25 01:32:44,987 : INFO : EPOCH 22 - PROGRESS: at 96.39% examples, 426077 words/s, in_qsize 6, out_qsize 0\n",
      "2018-11-25 01:32:45,423 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-11-25 01:32:45,427 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-11-25 01:32:45,428 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-11-25 01:32:45,428 : INFO : EPOCH - 22 : training on 3673661 raw words (2949666 effective words) took 6.5s, 452260 effective words/s\n",
      "2018-11-25 01:32:46,513 : INFO : EPOCH 23 - PROGRESS: at 17.14% examples, 264832 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:47,533 : INFO : EPOCH 23 - PROGRESS: at 34.61% examples, 271110 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:48,542 : INFO : EPOCH 23 - PROGRESS: at 51.47% examples, 304961 words/s, in_qsize 5, out_qsize 0\n",
      "2018-11-25 01:32:49,550 : INFO : EPOCH 23 - PROGRESS: at 65.13% examples, 358743 words/s, in_qsize 6, out_qsize 0\n"
     ]
    }
   ],
   "source": [
    "path_to_model = get_tmpfile(\"doc2vec_model_phr\")\n",
    "if(not Path(path_to_model).is_file()): \n",
    "    bigram = prepare_bigram(\"../corpus/merged.txt\")\n",
    "    documents = list(read_corpus(\"../corpus/merged.txt\",bigram))\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=250, min_count=2, epochs=500)\n",
    "    model.build_vocab(documents)\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model.save(path_to_model)\n",
    "    \n",
    "else:\n",
    "    model = Doc2Vec.load(path_to_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "umn_sim = pd.read_csv('UMNSRS_sim.csv')\n",
    "intrinsic_evaluate(umn_sim,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 23:24:00,202 : INFO : saving Doc2Vec object under C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model_phr, separately None\n",
      "2018-11-24 23:24:00,203 : INFO : storing np array 'syn1neg' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model_phr.trainables.syn1neg.npy\n",
      "2018-11-24 23:24:01,130 : INFO : storing np array 'vectors' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model_phr.wv.vectors.npy\n",
      "2018-11-24 23:24:02,105 : INFO : storing np array 'vectors_docs' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model_phr.docvecs.vectors_docs.npy\n",
      "2018-11-24 23:24:04,416 : INFO : saved C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model_phr\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
