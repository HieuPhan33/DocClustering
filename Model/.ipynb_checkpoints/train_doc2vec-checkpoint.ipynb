{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Program\\Anaconda\\envs\\tensorflow\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports needed and logging\n",
    "import zipfile\n",
    "import gensim \n",
    "import logging\n",
    "import io\n",
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.test.utils import get_tmpfile\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'york', 'is', 'disabled_prop']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_corpus(fname, bigram=None):\n",
    "    with open(fname,encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            line = re.sub(\"-\",\"_\",str(line))\n",
    "            list_of_words = gensim.utils.simple_preprocess(line)\n",
    "            if(bigram is not None):\n",
    "                # Further detect phrases\n",
    "                list_of_words = bigram[list_of_words]\n",
    "            yield list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus_zip(fname, tokens_only=False):\n",
    "    with zipfile.ZipFile(fname, \"r\") as z:\n",
    "        for name in z.namelist():\n",
    "            with z.open(name) as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    line = re.sub(\"-\",\"_\",str(line))\n",
    "                    if tokens_only:\n",
    "                        yield gensim.utils.simple_preprocess(line)\n",
    "                    else:\n",
    "                        # For training data, add tags\n",
    "                        yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(line), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = get_tmpfile(\"doc2vec_model_phr\")\n",
    "if(not Path(path_to_model).is_file()): \n",
    "    documents = list(read_corpus(\"../corpus/merged.txt\"))\n",
    "    model = gensim.models.doc2vec.Doc2Vec(vector_size=250, min_count=2, epochs=500)\n",
    "    model.build_vocab(documents)\n",
    "    %time model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)\n",
    "    model.save(path_to_model)\n",
    "    \n",
    "else:\n",
    "    model = Doc2Vec.load(path_to_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 12:01:33,358 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    }
   ],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(300):\n",
    "    inferred_vector = model.infer_vector(documents[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "    \n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-11-24 21:35:41,705 : INFO : saving Doc2Vec object under C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model, separately None\n",
      "2018-11-24 21:35:41,705 : INFO : storing np array 'syn1neg' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model.trainables.syn1neg.npy\n",
      "2018-11-24 21:35:41,901 : INFO : storing np array 'vectors' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model.wv.vectors.npy\n",
      "2018-11-24 21:35:42,114 : INFO : storing np array 'vectors_docs' to C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model.docvecs.vectors_docs.npy\n",
      "2018-11-24 21:35:42,884 : INFO : saved C:\\Users\\AD\\AppData\\Local\\Temp\\doc2vec_model\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
